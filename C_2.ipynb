{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlsxDbpDRQ+sQ3O+spFpYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkraman06/coursera_final-_project/blob/main/C_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Functional API\n",
        "\n"
      ],
      "metadata": {
        "id": "5zLvTuZTSz1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg_jnVudOCbo"
      },
      "outputs": [],
      "source": [
        "input_layer = Input(shape=(20,))\n",
        "print(input_layer)\n",
        "hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
        "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data (in practice, use real dataset)\n",
        "\n",
        "import numpy as np\n",
        "X_train = np.random.rand(1000, 20)\n",
        "y_train = np.random.randint(2, size=(1000, 1))\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "hxz2bR5EUBgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test data (in practice, use real dataset)\n",
        "\n",
        "X_test = np.random.rand(200, 20)\n",
        "y_test = np.random.randint(2, size=(200, 1))\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "HuFonSO4UHMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout and Batch Normalization"
      ],
      "metadata": {
        "id": "qq_0sJJ4ULwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(20,))\n",
        "\n",
        "# Add a hidden layer\n",
        "hidden_layer = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# Add a Dropout layer\n",
        "dropout_layer = Dropout(rate=0.5)(hidden_layer)\n",
        "\n",
        "# Add another hidden layer after Dropout\n",
        "hidden_layer2 = Dense(64, activation='relu')(dropout_layer)\n",
        "\n",
        "# Define the output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "XhiTX2AsUInB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization in Keras"
      ],
      "metadata": {
        "id": "eD2_EtksUQh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(20,))\n",
        "\n",
        "# Add a hidden layer\n",
        "hidden_layer = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# Add a BatchNormalization layer\n",
        "batch_norm_layer = BatchNormalization()(hidden_layer)\n",
        "\n",
        "# Add another hidden layer after BatchNormalization\n",
        "hidden_layer2 = Dense(64, activation='relu')(batch_norm_layer)\n",
        "\n",
        "# Define the output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "M78siQpdUVOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom Layers and Models"
      ],
      "metadata": {
        "id": "hlZ8HxD_UYBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom layer\n",
        "\n",
        "class CustomDenseLayer(Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(CustomDenseLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)\n",
        "\n",
        "# Integrate the custom layer into a model\n",
        "\n",
        "from tensorflow.keras.layers import Softmax\n",
        "\n",
        "# Define the model with Softmax in the output layer\n",
        "model = Sequential([\n",
        "    CustomDenseLayer(128),\n",
        "    CustomDenseLayer(10),  # Hidden layer with ReLU activation\n",
        "    Softmax()              # Output layer with Softmax activation for multi-class classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "lyV_IYN4UgfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "print(\"Model summary before building:\")\n",
        "model.summary()\n",
        "\n",
        "# Build the model to show parameters\n",
        "model.build((1000, 20))\n",
        "print(\"\\nModel summary after building:\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-ds83KeLU3RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "import numpy as np\n",
        "\n",
        "# Generate random data\n",
        "x_train = np.random.random((1000, 20))\n",
        "y_train = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "aDxMM7nJU6X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "# Generate random test data\n",
        "x_test = np.random.random((200, 20))\n",
        "y_test = np.random.randint(10, size=(200, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss}')"
      ],
      "metadata": {
        "id": "wTZ51TPxVEBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Model Architecture\n",
        "!pip install pydot graphviz\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Visualize the model architecture\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "AlZ0JEb5VJqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Dropout Layer\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Modify the model to include a Dropout layer\n",
        "model = Sequential([\n",
        "    CustomDenseLayer(64),\n",
        "    Dropout(0.5),\n",
        "    CustomDenseLayer(10)\n",
        "])\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train the model again\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "VpYEhS2lVSjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjust the Number of Units in Custom Layer\n",
        "# Define a custom layer with 128 units\n",
        "class CustomDenseLayer(Layer):\n",
        "    def __init__(self, units=128):\n",
        "        super(CustomDenseLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)\n",
        "\n",
        "# Integrate the new custom layer into a model\n",
        "model = Sequential([\n",
        "    CustomDenseLayer(128),\n",
        "    CustomDenseLayer(10)\n",
        "])\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train the model again\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "aVQedOS0Vd8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced Data Augmentation with Keras"
      ],
      "metadata": {
        "id": "LxuaOshpVlbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow==2.16.2 matplotlib==3.9.1 scipy\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load CIFAR-10 dataset for training images\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the pixel values for augmentation\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Display a sample of the training images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RSAPBTjMVlB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create sample.jpg for the Lab\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Create a blank white image\n",
        "image = Image.new('RGB', (224, 224), color = (255, 255, 255))\n",
        "\n",
        "# Draw a red square\n",
        "draw = ImageDraw.Draw(image)\n",
        "draw.rectangle([(50, 50), (174, 174)], fill=(255, 0, 0))\n",
        "\n",
        "# Save the image\n",
        "image.save('sample.jpg')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c_-BMtQdVuub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# Load a sample image\n",
        "img_path = 'sample.jpg'\n",
        "img = load_img(img_path)\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)"
      ],
      "metadata": {
        "id": "b13NoIa7V1I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic data augmentation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# Load the sample image\n",
        "img_path = 'sample.jpg'\n",
        "img = load_img(img_path)\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Create an instance of ImageDataGenerator with basic augmentations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate batches of augmented images\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(batch[0].astype('uint8'))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B8HXM2mYV6cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of ImageDataGenerator with normalization options\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    samplewise_center=True,\n",
        "    samplewise_std_normalization=True\n",
        ")\n",
        "\n",
        "# Load the sample image again and fit the generator (normally done on the training set)\n",
        "datagen.fit(x)\n",
        "\n",
        "# Generate batches of normalized images\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(batch[0].astype('uint8'))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O6vYIfUJWDIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom data augmentation function\n",
        "def add_random_noise(image):\n",
        "    noise = np.random.normal(0, 0.1, image.shape)\n",
        "    return image + noise\n",
        "\n",
        "# Create an instance of ImageDataGenerator with the custom augmentation\n",
        "datagen = ImageDataGenerator(preprocessing_function=add_random_noise)\n",
        "\n",
        "# Generate batches of augmented images with noise\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(batch[0].astype('uint8'))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T1SvLODOWF48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing multiple augmented versions of the same image\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, batch in enumerate(datagen.flow(x, batch_size=1)):\n",
        "    if i >= 4:  # Show only 4 versions\n",
        "        break\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(batch[0].astype('uint8'))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rhc7MMacWHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transfer Learning Implementation"
      ],
      "metadata": {
        "id": "AptQMf1ZWVhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the VGG16 model pre-trained on ImageNet\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "# Create a new model and add the base model and new layers\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Change to the number of classes you have\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Create Placeholder Images\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('sample_data/class_a', exist_ok=True)\n",
        "os.makedirs('sample_data/class_b', exist_ok=True)\n",
        "\n",
        "# Create 10 sample images for each class\n",
        "for i in range(10):\n",
        "    # Create a blank white image for class_a\n",
        "    img = Image.fromarray(np.ones((224, 224, 3), dtype=np.uint8) * 255)\n",
        "    img.save(f'sample_data/class_a/img_{i}.jpg')\n",
        "\n",
        "    # Create a blank black image for class_b\n",
        "    img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "    img.save(f'sample_data/class_b/img_{i}.jpg')\n",
        "\n",
        "print(\"Sample images created in 'sample_data/'\")\n"
      ],
      "metadata": {
        "id": "0Hs3Lk4NWKFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'sample_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Verify if the generator has loaded images correctly\n",
        "print(f\"Found {train_generator.samples} images belonging to {train_generator.num_classes} classes.\")\n",
        "\n",
        "# Train the model\n",
        "if train_generator.samples > 0:\n",
        "    model.fit(train_generator, epochs=10)"
      ],
      "metadata": {
        "id": "ek4Dx_-AWjfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the base model\n",
        "\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model again\n",
        "model.fit(train_generator, epochs=10)"
      ],
      "metadata": {
        "id": "erTVIihEWmpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Transformers for Text Generation"
      ],
      "metadata": {
        "id": "Kx5KUJr6Wvka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "path_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Preview the dataset\n",
        "print(text[:1000])\n",
        "\n",
        "\n",
        "# Preprocess the dataset\n",
        "vocab_size = 10000\n",
        "seq_length = 100\n",
        "\n",
        "# Adapt TextVectorization to full text\n",
        "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int')\n",
        "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "# Vectorize the text\n",
        "vectorized_text = vectorizer([text])[0]\n",
        "print(\"Vectorized text shape:\", vectorized_text.shape)\n",
        "print(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10])\n"
      ],
      "metadata": {
        "id": "A53q66afWxhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(text, seq_length):\n",
        "    input_seqs = []\n",
        "    target_seqs = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        input_seq = text[i:i + seq_length]\n",
        "        target_seq = text[i + 1:i + seq_length + 1]\n",
        "        input_seqs.append(input_seq)\n",
        "        target_seqs.append(target_seq)\n",
        "    return np.array(input_seqs), np.array(target_seqs)\n",
        "\n",
        "# Generate sequences\n",
        "X, Y = create_sequences(vectorized_text.numpy(), seq_length)\n",
        "\n",
        "# Check if sequences are correctly generated\n",
        "print(\"Number of sequences generated:\", len(X))\n",
        "print(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\")\n",
        "\n",
        "# Check if X and Y are not empty\n",
        "assert X.size > 0, \"Input data X is empty\"\n",
        "assert Y.size > 0, \"Target data Y is empty\"\n",
        "X = tf.convert_to_tensor(X)\n",
        "Y = tf.convert_to_tensor(Y)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of Y:\", Y.shape)\n"
      ],
      "metadata": {
        "id": "UAgomvFZXGdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerModel(Model):  # Model is now properly imported\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
        "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
        "        self.dense = Dense(vocab_size)\n",
        "\n",
        "    def positional_encoding(self, seq_length, embed_dim):\n",
        "        angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, embed_dim):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        x = self.embedding(inputs)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)  # Pass training argument correctly\n",
        "        output = self.dense(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "YQE4PQ2hXJr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "\n",
        "# Build the Transformer model\n",
        "model = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
        "\n",
        "# Provide input shape to build the model by passing a dummy input with maxval specified\n",
        "_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Eve4cD-GXLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for training visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping callback to stop training if the loss doesn't improve\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Train the transformer model on the full input and target sequences\n",
        "history = model.fit(X, Y, epochs=20, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Plot training loss to monitor model performance over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "snBkMdYuXN-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=100, temperature=1.0):\n",
        "    # Convert the start string to a vectorized format\n",
        "    input_eval = vectorizer([start_string]).numpy()\n",
        "\n",
        "    # Ensure the input length is the same as the model's expected input shape\n",
        "    if input_eval.shape[1] < seq_length:\n",
        "        # Pad the input if it's shorter than the expected sequence length\n",
        "        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n",
        "        input_eval = np.concatenate((padding, input_eval), axis=1)\n",
        "    elif input_eval.shape[1] > seq_length:\n",
        "        # Truncate the input if it's longer than the expected sequence length\n",
        "        input_eval = input_eval[:, -seq_length:]\n",
        "\n",
        "    input_eval = tf.convert_to_tensor(input_eval)\n",
        "\n",
        "    # Initialize an empty list to store generated text\n",
        "    text_generated = []\n",
        "\n",
        "    # Start generating text\n",
        "    for i in range(num_generate):\n",
        "        # Make predictions using the model\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove only the batch dimension, keep the logits as 2D (batch_size, vocab_size)\n",
        "        predictions = predictions[0]  # This should be of shape [vocab_size]\n",
        "\n",
        "        # Apply temperature to predictions\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Use a categorical distribution to predict the next word\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        # Update the input tensor to include the predicted word, maintaining the sequence length\n",
        "        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)  # Append predicted token\n",
        "        input_eval = input_eval[:, -seq_length:]  # Keep only the last `seq_length` tokens\n",
        "        input_eval = tf.convert_to_tensor(input_eval)  # Convert back to tensor\n",
        "\n",
        "        # Append the predicted word to the generated text\n",
        "        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n",
        "\n",
        "    # Return the generated text starting from the initial seed\n",
        "    return start_string + ' ' + ' '.join(text_generated)\n",
        "\n",
        "# Generate text with temperature control\n",
        "start_string = \"To be, or not to be\"\n",
        "generated_text = generate_text(model, start_string, temperature=0.7)  # Lower temperature for more focused predictions\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "Q6E4jb2-XREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Diffusion Models"
      ],
      "metadata": {
        "id": "4LwoePP8XqU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow-cpu==2.16.2\n",
        "\n",
        "import os\n",
        "# Suppress oneDNN optimizations and lower TensorFlow logging level\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
      ],
      "metadata": {
        "id": "Y4z0RltZXr72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the data set\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Expand dimensions to match the input shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# Add noise to the data\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip the values to be within the range [0, 1]\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n"
      ],
      "metadata": {
        "id": "5PBWdKtxXv73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the diffusion model architecture with reduced complexity\n",
        "input_layer = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)  # Reduced filters\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='relu')(x)  # Reduced size\n",
        "x = Dense(28*28*32, activation='relu')(x)  # Reduced size\n",
        "x = Reshape((28, 28, 32))(x)\n",
        "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
        "x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
        "output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "diffusion_model = Model(input_layer, output_layer)\n",
        "\n",
        "# Compile the model with mixed precision and a different loss function\n",
        "diffusion_model.compile(optimizer='adam', loss='mean_squared_error')  # Using MSE for regression tasks\n",
        "\n",
        "# Summary of the optimized model\n",
        "diffusion_model.summary()\n"
      ],
      "metadata": {
        "id": "iPUb-PU-Xy4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache and prefetch the data using TensorFlow data pipelines for faster loading\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_noisy, x_train))\n",
        "train_dataset = train_dataset.cache().batch(64).prefetch(tf.data.AUTOTUNE)  # Reduced batch size\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test_noisy, x_test))\n",
        "val_dataset = val_dataset.cache().batch(64).prefetch(tf.data.AUTOTUNE)  # Reduced batch size\n"
      ],
      "metadata": {
        "id": "HRDEBe5CX1f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement early stopping based on validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and smaller batch size\n",
        "diffusion_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=3,\n",
        "    shuffle=True,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "_jj9ulmGX5Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict the denoised images\n",
        "denoised_images = diffusion_model.predict(x_test_noisy)\n",
        "\n",
        "# Visualize the results\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display noisy\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display denoised\n",
        "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
        "    plt.imshow(denoised_images[i].reshape(28, 28), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bfisdxhMX8Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the model\n",
        "for layer in diffusion_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again\n",
        "diffusion_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model again\n",
        "diffusion_model.fit(x_train_noisy, x_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(x_test_noisy, x_test))\n"
      ],
      "metadata": {
        "id": "Bbx-blvpX9DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Autoencoders"
      ],
      "metadata": {
        "id": "d8QVNY7AYDFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "metadata": {
        "id": "Yn6_HB3jYe5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Encoder\n",
        "input_layer = Input(shape=(784,))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# Bottleneck\n",
        "bottleneck = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(64, activation='relu')(bottleneck)\n",
        "output_layer = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Summary of the model\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "ehdcyzggYh6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Autoencoder\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "RH6K1cLvYlK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict the test data\n",
        "reconstructed = autoencoder.predict(x_test)\n",
        "\n",
        "# Visualize the results\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BXEGjvpHZWji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the encoder\n",
        "for layer in autoencoder.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model again\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n"
      ],
      "metadata": {
        "id": "AAryGF6-Zc46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Denoising Images with Autoencoder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add noise to the data\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Train the autoencoder with noisy data\n",
        "autoencoder.fit(\n",
        "    x_train_noisy, x_train,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_noisy, x_test)\n",
        ")\n",
        "\n",
        "# Denoise the test images\n",
        "reconstructed_noisy = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Visualize the results\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # Display noisy images\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display denoised images\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display original images\n",
        "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C7cL3T0OZh1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop GANs Using Keras"
      ],
      "metadata": {
        "id": "Mrjl-n51b4Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import warnings\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [-1, 1]\n",
        "x_train = x_train.astype('float32') / 127.5 - 1.\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Print the shape of the data\n",
        "print(x_train.shape)\n"
      ],
      "metadata": {
        "id": "wQVzxqunb1Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the generator mode\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(28 * 28 * 1, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    return model\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator()\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "01ksJUNpb8ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the discriminator model\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "ITFgtqwTcC06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the GAN Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Create the GAN by stacking the generator and the discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(100,))\n",
        "    generated_image = generator(gan_input)\n",
        "    gan_output = discriminator(generated_image)\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "# Build the GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.summary()\n"
      ],
      "metadata": {
        "id": "RO7UNigQcKlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the GAN\n",
        "# Define and compile the discriminator model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Flatten\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and recompile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "P0VTE19UcQIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "sample_interval = 10\n",
        "\n",
        "# Adversarial ground truths\n",
        "real = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Train the discriminator\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    real_images = x_train[idx]\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    d_loss_real = discriminator.train_on_batch(real_images, real)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_images, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    g_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "    # Print the progress\n",
        "    if epoch % sample_interval == 0:\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]}] [D accuracy: {100 * d_loss[1]}%] [G loss: {g_loss}]\")\n"
      ],
      "metadata": {
        "id": "KJP7FpNicZDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assessing the Quality of Generated Images\n",
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sample_images(generator, epoch, num_images=25):\n",
        "    noise = np.random.normal(0, 1, (num_images, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "    fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
        "    count = 0\n",
        "\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    plt.show()\n",
        "\n",
        "# Sample images at the end of training\n",
        "sample_images(generator, epochs)\n"
      ],
      "metadata": {
        "id": "bN_H-kercbyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the discriminator accuracy on real vs. fake images\n",
        "noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "generated_images = generator.predict(noise)\n",
        "\n",
        "# Evaluate the discriminator on real images\n",
        "real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
        "d_loss_real = discriminator.evaluate(real_images, np.ones((batch_size, 1)), verbose=0)\n",
        "\n",
        "# Evaluate the discriminator on fake images\n",
        "d_loss_fake = discriminator.evaluate(generated_images, np.zeros((batch_size, 1)), verbose=0)\n",
        "\n",
        "print(f\"Discriminator Accuracy on Real Images: {d_loss_real[1] * 100:.2f}%\")\n",
        "print(f\"Discriminator Accuracy on Fake Images: {d_loss_fake[1] * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "52VRi0Vpchcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training Loops in Keras"
      ],
      "metadata": {
        "id": "m-S6UsBEcrNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "metadata": {
        "id": "dz0YLfg1cq6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])\n"
      ],
      "metadata": {
        "id": "TD2uvJW0cvxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define Loss Function and Optimizer\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n"
      ],
      "metadata": {
        "id": "y-yNE0Stc00R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Implement the Custom Training Loop\n",
        "\n",
        "epochs = 2\n",
        "# train_dataset = train_dataset.repeat(epochs)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)  # Forward pass\n",
        "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
        "\n",
        "        # Compute gradients and update weights\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Logging the loss every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
      ],
      "metadata": {
        "id": "ZgGovigpc1q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Accuracy Metric\n"
      ],
      "metadata": {
        "id": "Cs6VGqzlc7NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create a batched dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "metadata": {
        "id": "bUUbL4u2c_IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
        "])\n"
      ],
      "metadata": {
        "id": "3pFhzAJ1dBYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define Loss Function, Optimizer, and Metric\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
      ],
      "metadata": {
        "id": "PG30p_TqdDjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Implement the Custom Training Loop with Accuracy\n",
        "\n",
        "epochs = 5  # Number of epochs for training\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass: Compute predictions\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            # Compute loss\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # Apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()\n"
      ],
      "metadata": {
        "id": "QI-FSnbVdFWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Callback for Advanced Logging"
      ],
      "metadata": {
        "id": "V_h4Mx4-dKmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create a batched dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "metadata": {
        "id": "1ASBSVAAdHdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
        "])\n"
      ],
      "metadata": {
        "id": "RicC6EE4dObC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define Loss Function, Optimizer, and Metric\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
      ],
      "metadata": {
        "id": "wylS9krMdReQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# Step 4: Implement the Custom Callback\n",
        "class CustomCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
      ],
      "metadata": {
        "id": "7R0-GrV9dTaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
        "\n",
        "epochs = 2\n",
        "custom_callback = CustomCallback()  # Initialize the custom callback\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass: Compute predictions\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            # Compute loss\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # Apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Call the custom callback at the end of each epoch\n",
        "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
      ],
      "metadata": {
        "id": "3QZ9rRCodXJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Hidden Layers"
      ],
      "metadata": {
        "id": "kRYRdfkndZ6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
        "\n",
        "# Define hidden layers\n",
        "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
        "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
      ],
      "metadata": {
        "id": "ufYZy7uKdftC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "xUn7qrjLdjkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Redefine the Model for 20 features\n",
        "model = Sequential([\n",
        "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
        "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 2: Generate Example Data\n",
        "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
        "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "9ulVWHHkdsr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test data (in practice, use real dataset)\n",
        "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
        "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print test loss and accuracy\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "iVLPMshldtkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning with Keras Tuner"
      ],
      "metadata": {
        "id": "sCFbcCE6d1Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only\n",
        "\n"
      ],
      "metadata": {
        "id": "cTpXqJ69d2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')"
      ],
      "metadata": {
        "id": "ENCay48xd5oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model-building function\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "_aOl-rbxd8M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomSearch Tuner\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "qU12wlYOd-Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Display a summary of the results\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "ks_hT54CeBTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing and using the best hyperparameters"
      ],
      "metadata": {
        "id": "uObNJmSHeITY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Retrieve the best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"\n",
        "\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Step 2: Build and Train the Model with Best Hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_val, y_val)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "28zQm4RjeEZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkhLVEgTeLjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}